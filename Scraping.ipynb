{"cells":[{"cell_type":"markdown","source":["Objective of this notebook is to collect latest COVID-19 dataset using web scraping method and convert it into presentable format"],"metadata":{}},{"cell_type":"markdown","source":["<b>Selenium for web scraping</b></br>\nSelenium is an automation testing framework for web application/ website testing and navigation.</br>\nThe notebook explains how the data can be scraped from websites. In this notebook, we are scraping data from worldometer website which provides up-to-date COVID-19 data.</br>\nLet us start with installing Selenium wedriver"],"metadata":{}},{"cell_type":"code","source":["%sh \n/databricks/python3/bin/pip3 install selenium"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Requirement already satisfied: selenium in /databricks/python3/lib/python3.7/site-packages (3.141.0)\nRequirement already satisfied: urllib3 in /databricks/python3/lib/python3.7/site-packages (from selenium) (1.24.1)\nYou are using pip version 19.0.3, however version 20.2b1 is available.\nYou should consider upgrading via the &#39;pip install --upgrade pip&#39; command.\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["#Import webdriver\nfrom selenium import webdriver"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["<b>Download Chrome driver for linux</b>"],"metadata":{}},{"cell_type":"code","source":["%sh\nwget https://chromedriver.storage.googleapis.com/72.0.3626.7/chromedriver_linux64.zip -O /tmp/chromedriver_linux64.zip"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">--2020-06-05 16:15:07--  https://chromedriver.storage.googleapis.com/72.0.3626.7/chromedriver_linux64.zip\nResolving chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)... 172.217.13.240, 2607:f8b0:4004:804::2010\nConnecting to chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)|172.217.13.240|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 5035783 (4.8M) [application/zip]\nSaving to: ‘/tmp/chromedriver_linux64.zip’\n\n     0K .......... .......... .......... .......... ..........  1% 26.4M 0s\n    50K .......... .......... .......... .......... ..........  2% 39.0M 0s\n   100K .......... .......... .......... .......... ..........  3% 47.6M 0s\n   150K .......... .......... .......... .......... ..........  4% 90.2M 0s\n   200K .......... .......... .......... .......... ..........  5% 59.5M 0s\n   250K .......... .......... .......... .......... ..........  6% 78.9M 0s\n   300K .......... .......... .......... .......... ..........  7% 93.4M 0s\n   350K .......... .......... .......... .......... ..........  8% 61.6M 0s\n   400K .......... .......... .......... .......... ..........  9% 97.2M 0s\n   450K .......... .......... .......... .......... .......... 10% 56.2M 0s\n   500K .......... .......... .......... .......... .......... 11%  103M 0s\n   550K .......... .......... .......... .......... .......... 12% 80.7M 0s\n   600K .......... .......... .......... .......... .......... 13% 87.1M 0s\n   650K .......... .......... .......... .......... .......... 14% 97.0M 0s\n   700K .......... .......... .......... .......... .......... 15% 77.1M 0s\n   750K .......... .......... .......... .......... .......... 16%  105M 0s\n   800K .......... .......... .......... .......... .......... 17% 64.6M 0s\n   850K .......... .......... .......... .......... .......... 18% 78.8M 0s\n   900K .......... .......... .......... .......... .......... 19% 68.8M 0s\n   950K .......... .......... .......... .......... .......... 20%  105M 0s\n  1000K .......... .......... .......... .......... .......... 21%  110M 0s\n  1050K .......... .......... .......... .......... .......... 22%  287M 0s\n  1100K .......... .......... .......... .......... .......... 23%  260M 0s\n  1150K .......... .......... .......... .......... .......... 24%  314M 0s\n  1200K .......... .......... .......... .......... .......... 25%  262M 0s\n  1250K .......... .......... .......... .......... .......... 26%  325M 0s\n  1300K .......... .......... .......... .......... .......... 27%  280M 0s\n  1350K .......... .......... .......... .......... .......... 28%  141M 0s\n  1400K .......... .......... .......... .......... .......... 29%  217M 0s\n  1450K .......... .......... .......... .......... .......... 30%  258M 0s\n  1500K .......... .......... .......... .......... .......... 31%  294M 0s\n  1550K .......... .......... .......... .......... .......... 32%  315M 0s\n  1600K .......... .......... .......... .......... .......... 33%  270M 0s\n  1650K .......... .......... .......... .......... .......... 34%  153M 0s\n  1700K .......... .......... .......... .......... .......... 35%  314M 0s\n  1750K .......... .......... .......... .......... .......... 36%  228M 0s\n  1800K .......... .......... .......... .......... .......... 37%  194M 0s\n  1850K .......... .......... .......... .......... .......... 38%  216M 0s\n  1900K .......... .......... .......... .......... .......... 39%  214M 0s\n  1950K .......... .......... .......... .......... .......... 40%  170M 0s\n  2000K .......... .......... .......... .......... .......... 41%  175M 0s\n  2050K .......... .......... .......... .......... .......... 42%  199M 0s\n  2100K .......... .......... .......... .......... .......... 43%  226M 0s\n  2150K .......... .......... .......... .......... .......... 44%  348M 0s\n  2200K .......... .......... .......... .......... .......... 45%  228M 0s\n  2250K .......... .......... .......... .......... .......... 46%  280M 0s\n  2300K .......... .......... .......... .......... .......... 47%  234M 0s\n  2350K .......... .......... .......... .......... .......... 48%  274M 0s\n  2400K .......... .......... .......... .......... .......... 49%  129M 0s\n  2450K .......... .......... .......... .......... .......... 50%  236M 0s\n  2500K .......... .......... .......... .......... .......... 51%  311M 0s\n  2550K .......... .......... .......... .......... .......... 52%  316M 0s\n  2600K .......... .......... .......... .......... .......... 53%  189M 0s\n  2650K .......... .......... .......... .......... .......... 54%  147M 0s\n  2700K .......... .......... .......... .......... .......... 55%  172M 0s\n  2750K .......... .......... .......... .......... .......... 56%  293M 0s\n  2800K .......... .......... .......... .......... .......... 57%  266M 0s\n  2850K .......... .......... .......... .......... .......... 58%  326M 0s\n  2900K .......... .......... .......... .......... .......... 59%  336M 0s\n  2950K .......... .......... .......... .......... .......... 61%  318M 0s\n  3000K .......... .......... .......... .......... .......... 62%  207M 0s\n  3050K .......... .......... .......... .......... .......... 63%  225M 0s\n  3100K .......... .......... .......... .......... .......... 64%  266M 0s\n  3150K .......... .......... .......... .......... .......... 65%  179M 0s\n  3200K .......... .......... .......... .......... .......... 66%  262M 0s\n  3250K .......... .......... .......... .......... .......... 67%  350M 0s\n  3300K .......... .......... .......... .......... .......... 68% 68.4M 0s\n  3350K .......... .......... .......... .......... .......... 69%  176M 0s\n  3400K .......... .......... .......... .......... .......... 70%  204M 0s\n  3450K .......... .......... .......... .......... .......... 71%  264M 0s\n  3500K .......... .......... .......... .......... .......... 72%  335M 0s\n  3550K .......... .......... .......... .......... .......... 73% 63.9M 0s\n  3600K .......... .......... .......... .......... .......... 74%  170M 0s\n  3650K .......... .......... .......... .......... .......... 75%  162M 0s\n  3700K .......... .......... .......... .......... .......... 76%  257M 0s\n  3750K .......... .......... .......... .......... .......... 77%  202M 0s\n  3800K .......... .......... .......... .......... .......... 78% 28.8M 0s\n  3850K .......... .......... .......... .......... .......... 79%  177M 0s\n  3900K .......... .......... .......... .......... .......... 80%  177M 0s\n  3950K .......... .......... .......... .......... .......... 81%  268M 0s\n  4000K .......... .......... .......... .......... .......... 82%  279M 0s\n  4050K .......... .......... .......... .......... .......... 83% 52.7M 0s\n  4100K .......... .......... .......... .......... .......... 84%  143M 0s\n  4150K .......... .......... .......... .......... .......... 85%  195M 0s\n  4200K .......... .......... .......... .......... .......... 86%  294M 0s\n  4250K .......... .......... .......... .......... .......... 87%  334M 0s\n  4300K .......... .......... .......... .......... .......... 88%  331M 0s\n  4350K .......... .......... .......... .......... .......... 89% 34.9M 0s\n  4400K .......... .......... .......... .......... .......... 90%  194M 0s\n  4450K .......... .......... .......... .......... .......... 91%  283M 0s\n  4500K .......... .......... .......... .......... .......... 92%  353M 0s\n  4550K .......... .......... .......... .......... .......... 93%  344M 0s\n  4600K .......... .......... .......... .......... .......... 94% 34.9M 0s\n  4650K .......... .......... .......... .......... .......... 95%  189M 0s\n  4700K .......... .......... .......... .......... .......... 96%  192M 0s\n  4750K .......... .......... .......... .......... .......... 97%  335M 0s\n  4800K .......... .......... .......... .......... .......... 98%  280M 0s\n  4850K .......... .......... .......... .......... .......... 99% 13.0M 0s\n  4900K .......... .......                                    100%  319M=0.04s\n\n2020-06-05 16:15:07 (118 MB/s) - ‘/tmp/chromedriver_linux64.zip’ saved [5035783/5035783]\n\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["<b>Setup and Installation</b>\n* Create chrome driver directory\n* Unzip Chrome\n* Add PPA repository for chrome driver\n* Check updates\n* Install Chrome"],"metadata":{}},{"cell_type":"code","source":["%sh mkdir /tmp/chromedriver"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">mkdir: cannot create directory ‘/tmp/chromedriver’: File exists\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["%sh\nunzip /tmp/chromedriver_linux64.zip -d /tmp/chromedriver/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Archive:  /tmp/chromedriver_linux64.zip\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["%sh\nsudo add-apt-repository ppa:canonical-chromium-builds/stage"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%sh\n/usr/bin/yes | sudo apt update"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%sh\n/usr/bin/yes | sudo apt install chromium-browser"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["<b>Test</b>"],"metadata":{}},{"cell_type":"code","source":["#Test\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument('--no-sandbox')\nchrome_options.add_argument('--headless')\nchrome_options.add_argument('--disable-dev-shm-usage')\nchrome_driver = \"/tmp/chromedriver/chromedriver\"\ndriver = webdriver.Chrome(executable_path=chrome_driver,options=chrome_options)\ndriver.get(\"https://www.google.com\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["<b>Web Scraping</b>"],"metadata":{}},{"cell_type":"code","source":["#Navigate to worldometer website\ndriver.get('https://www.worldometers.info/coronavirus/')"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#Go to website and search in the source code for the countries table\ntable = driver.find_element_by_xpath('//*[@id=\"main_table_countries_today\"]/tbody[1]')"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#Create pandas dataframe\nimport pandas as pd\ndf = pd.DataFrame([], columns = [\"Index\",\"Country\",\"Total Cases\",\"New Cases\",\"Total Deaths\",\"New Deaths\",\"Total Recovered\",\"Active Cases\",\"Serious/Critical\",\"Total Cases/ 1M pop\",\"Deaths/ 1M pop\",\"Total tests\",\"Tests/ 1M pop\",\"Population\",\"Extra\"])"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["#Iterate through elements and append each line to the dataframe\nfor row in table.find_elements_by_xpath(\".//tr\"):\n  line = [td.text for td in row.find_elements_by_xpath(\".//td\")]\n  df_len = len(df)\n  df.loc[df_len] = line"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["df"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["<b>View Data and Clean</b>"],"metadata":{}},{"cell_type":"code","source":["#Convert to spark dataframe\nsparkdf = spark.createDataFrame(df)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["display(sparkdf)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["#Data cleaning\nfrom pyspark.sql.functions import regexp_replace, col\ndata = sparkdf.filter(sparkdf.Country != \"\")\ndata = data.select(\n                     col(\"Index\").cast('Integer').alias('Index')\n                   , regexp_replace(regexp_replace(regexp_replace(\"Country\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").alias(\"Country\")\n                   , regexp_replace(regexp_replace(regexp_replace(\"Total Cases\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").cast('Integer').alias(\"TotalCases\")\n                   , regexp_replace(regexp_replace(regexp_replace(\"New Cases\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").cast('Integer').alias(\"NewCases\")\n                   , regexp_replace(regexp_replace(regexp_replace(\"Total Deaths\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").cast('Integer').alias(\"TotalDeaths\")\n                   , regexp_replace(regexp_replace(regexp_replace(\"New Deaths\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").cast('Integer').alias(\"NewDeaths\")\n                   , regexp_replace(regexp_replace(regexp_replace(\"Total Recovered\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").cast('Integer').alias(\"TotalRecovered\")\n                   , regexp_replace(regexp_replace(regexp_replace(\"Active Cases\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").cast('Integer').alias(\"ActiveCases\")\n                   , regexp_replace(regexp_replace(regexp_replace(\"Serious/Critical\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").cast('Integer').alias(\"Serious/Critical\")\n                   , regexp_replace(regexp_replace(regexp_replace(\"Total Cases/ 1M pop\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").cast('Float').alias(\"TotalCases/1Mpop\")\n                   , regexp_replace(regexp_replace(regexp_replace(\"Deaths/ 1M pop\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").cast('Float').alias(\"Deaths/1Mpop\")\n                   , regexp_replace(regexp_replace(regexp_replace(\"Total tests\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").cast('Integer').alias(\"TotalTests\")\n                   , regexp_replace(regexp_replace(regexp_replace(\"Tests/ 1M pop\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").cast('Float').alias(\"Tests/1Mpop\")\n                   , regexp_replace(regexp_replace(regexp_replace(\"Population\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").cast('Integer').alias(\"Population\"))                 \ndata = data.na.fill(0)\ndisplay(data)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["<b>Query Data</b>"],"metadata":{}},{"cell_type":"code","source":["#Create a temp view and query data using SQL\ndata.createOrReplaceTempView('Cases')"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%sql\n--Top 10 countries with most cases\nselect * from Cases where Index!=0 order by TotalCases desc limit 10"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["%sql\n--Top 10 countries with most tests/million population having more than 30000 cases\nselect * from Cases where Index!=0 and TotalCases > 30000 order by `Tests/1Mpop` desc limit 10"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["%sql\n--Countries having most fatality rate with more than 30000 Total cases detected\nselect Country, round((TotalDeaths/TotalCases)*100,2) as fatality from Cases where index!=0 and TotalCases > 30000 and round((TotalDeaths/TotalCases)*100,2)!=0 order by fatality desc limit 20"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["%sql\n--Countries having most recovery rate with more than 30000 Total cases detected\nselect Country, round((TotalRecovered/TotalCases)*100,2) as recovery from Cases where index!=0 and TotalCases > 30000 and round((TotalRecovered/TotalCases)*100,2)!=0 order by recovery desc limit 20"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["%sql\n--Total Deaths, Active Cases, Total Recovered for countries with most active cases\nselect Country, ActiveCases, TotalDeaths, TotalRecovered from Cases where index!=0 and TotalCases > 30000 order by ActiveCases desc limit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\ncom.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: Table or view not found: Cases; line 1 pos 62\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:749)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:730)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:723)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:723)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:663)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:112)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:109)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:109)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:101)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:101)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:137)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:79)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:115)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:83)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:696)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:716)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:88)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:34)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:34)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:141)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:396)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:373)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:373)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'cases' not found in database 'default';\n\tat org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n\tat org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.hive.client.HiveClient$class.getTable(HiveClient.scala:81)\n\tat org.apache.spark.sql.hive.client.PoolingHiveClient.getTable(PoolingHiveClient.scala:42)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getRawTable$1.apply(HiveExternalCatalog.scala:179)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getRawTable$1.apply(HiveExternalCatalog.scala:179)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1$$anonfun$apply$1.apply(HiveExternalCatalog.scala:144)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$maybeSynchronized(HiveExternalCatalog.scala:105)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1.apply(HiveExternalCatalog.scala:142)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:372)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:358)\n\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:140)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:178)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:767)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:767)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1$$anonfun$apply$1.apply(HiveExternalCatalog.scala:144)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$maybeSynchronized(HiveExternalCatalog.scala:105)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1.apply(HiveExternalCatalog.scala:142)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:372)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:358)\n\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:140)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:766)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:143)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:736)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog$1.apply(Analyzer.scala:746)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog$1.apply(Analyzer.scala:746)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:79)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:745)\n\t... 119 more\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:126)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:141)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:396)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:373)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:373)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)"]}}],"execution_count":31},{"cell_type":"markdown","source":["<b>Web scraping USA data</b>"],"metadata":{}},{"cell_type":"code","source":["#Navigate to worldometer website\ndriver.get('https://www.worldometers.info/coronavirus/country/us/')\n\n#Go to website and search in the source code for the countries table\ntable = driver.find_element_by_xpath('//*[@id=\"usa_table_countries_today\"]/tbody[1]')\n\n#Create pandas dataframe\nimport pandas as pd\nusadf = pd.DataFrame([], columns = [\"Country\",\"Total Cases\",\"New Cases\",\"Total Deaths\",\"New Deaths\",\"Active Cases\",\"Total Cases/ 1M pop\",\"Deaths/ 1M pop\",\"Total tests\",\"Tests/ 1M pop\",\"Source\",\"Projections\"])\n\n#Iterate through elements and append each line to the dataframe\nfor row in table.find_elements_by_xpath(\".//tr\"):\n  line = [td.text for td in row.find_elements_by_xpath(\".//td\")]\n  usa_df_len = len(usadf)\n  usadf.loc[usa_df_len] = line"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["usadf"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["usa_sparkdf = spark.createDataFrame(usadf)\nfrom pyspark.sql.functions import regexp_replace, col\nusadata = usa_sparkdf.filter(usa_sparkdf.Country != \"\")\nusadata = usadata.select(\n                     regexp_replace(regexp_replace(regexp_replace(\"Country\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").alias(\"Country\")\n                   , regexp_replace(regexp_replace(regexp_replace(\"Total Cases\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").cast('Integer').alias(\"TotalCases\")\n                   , regexp_replace(regexp_replace(regexp_replace(\"New Cases\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").cast('Integer').alias(\"NewCases\")\n                   , regexp_replace(regexp_replace(regexp_replace(\"Total Deaths\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").cast('Integer').alias(\"TotalDeaths\")\n                   , regexp_replace(regexp_replace(regexp_replace(\"New Deaths\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").cast('Integer').alias(\"NewDeaths\")\n                   , regexp_replace(regexp_replace(regexp_replace(\"Active Cases\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").cast('Integer').alias(\"ActiveCases\")\n                   , regexp_replace(regexp_replace(regexp_replace(\"Total Cases/ 1M pop\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").cast('Float').alias(\"TotalCases/1Mpop\")\n                   , regexp_replace(regexp_replace(regexp_replace(\"Deaths/ 1M pop\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").cast('Float').alias(\"Deaths/1Mpop\")\n                   , regexp_replace(regexp_replace(regexp_replace(\"Total tests\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").cast('Integer').alias(\"TotalTests\")\n                   , regexp_replace(regexp_replace(regexp_replace(\"Tests/ 1M pop\", \",\",\"\"), \"\\\\+\", \"\"), \"N/A\", \"\").cast('Float').alias(\"Tests/1Mpop\"))    \nusadata = usadata.na.fill(0)\ndisplay(usadata)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["#Create a temp view\nusadata.createOrReplaceTempView('USCases')"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["%sql\nselect * from USCases where Country!=\"USA Total\" order by TotalCases desc limit 10"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["%sql\n--Total Deaths, Active Cases, Total Recovered \nselect Country, ActiveCases, TotalDeaths from USCases where Country!=\"USA Total\" and TotalCases > 30000 order by ActiveCases desc limit 20"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["<b>Access for external tools</b>\nCreate database tables so that the data can be accessed by external tools"],"metadata":{}},{"cell_type":"code","source":["data.write.saveAsTable(\"WorldCases\")\nusadata.write.saveAsTable(\"USACases\")"],"metadata":{},"outputs":[],"execution_count":40}],"metadata":{"name":"Scraping","notebookId":3629550730988524},"nbformat":4,"nbformat_minor":0}
